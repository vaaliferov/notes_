> questions
> * how does l1 select features?
> * why should we normalize data?
> * what metric should you use and why (regression)?
> * why can't we use linear regression for classification?
> * why is an ensemble (bagging / boosting) better than a tree?

> recommender systems: singular value decomposition (svd), latent semantic analysis (lsa), bm25

> architectures: [microsoft] resnet, [oxford] vgg, [ultralytics] yolo5, [google] inception, efficient-net, efficient-det, bert, t5, transformer-xl, xlnet [openai] gpt, clip, glide, dall-e, [deepmind] flamingo, gato, [ai21] jurassic-1, jurassic-x, [facebook / meta] roberta, bart, opt-175b

> cases: classification, regression, ranking, clustering, anomaly detection, dimensionality reduction, image classification, object detection, segmentation, image generation, spam detection, sentiment analysis, named-entity recognition (ner), part-of-speech tagging (pos-tagging), dependency parsing, constituency parsing, visual question-answering (vqa), automatic speech recognition (asr), speech to text (stt), text to speech (tts), dialogue systems, virtual assistants, question answering, visual question answering, text to image, image captioning, named entity recognition, speech enhancement, denoising, noise canceling, music source separation, voice cloning, writing style recognition, machine translation, speech correction, churn prediction, text summarization, toxic comments detection, handwriting recognition

> linux: bash, sudo, history, alias, gcc, git, docker, systemd, cron, chmod, chown, cd, mkdir, rm, mv, cp, ln, pwd, ls, du, df, fdisk, sed, grep, ack, ag, ps, top, htop, btop, wget, curl, youtube-dl, aria2c, tar, cat, head, tail, less, man, whereis, which, whatis, file, ssh, ssh-keygen, ssh-copy-id, mosh, scp, rsync, tmux, vim, ffmpeg, ventoy, ncdu, ncspot, ncmpc, gdown, obs, alacritty, kitty, nnn ||| sublime, vscode, mpv, vlc, transmission, deluge

> libs & frameworks: numpy, pandas, sklearn, umap, catboost, lightgbm, xgboost, jupyter, pytorch, tensorflow, keras, opencv, tesseract, matplotlib, seaborn, plotly, pydot, pandas-profiling, scipy, streamlit, python-telegram-bot, scrapy, beautifulsoup, selenium, nltk, gensim, youtokentome, sentencepiece, spacy, transformers & datasets (hugging face), pymorphy2, rnnmorph, pymystem3, natasha, razdel, folium, faiss, prophet, pipenv, poetry, optuna

> devops & mlops: linux bash, tmux, git, docker, dvc, airflow, mlflow, databricks, clickhouse, poetry, snakemake, spark, hadoop, fiddler, evidently, directed acyclic graph (dag, pipeline)

> numpy & pandas: indexing, slicing, filtering, groupby, apply, transform, agg

> datasets: imagenet, oscar, imdb, twitter, timit, voxceleb, wall street journal (wsj), common voice, librespeech, ruslan, glue, super-glue, squad

> neural networks: artificial neural network (ann), convolutional neural network (cnn), recurrent neural network (rnn), gated recurrent unit (gru), long short-term memory (lstm), one-to-one ... many-to-many, discriminative models, generative models, flow-based models, diffusion models, partial derivative, gradient descent, chain rule, forward / back propagation, back propagation through time (bptt), data normalization, batch norm, layer norm, optimizers (sgd, momentum, adagrad, adadelta, rmsprop, adam), learning rate scheduling, ff-layer, lstm layer, gru layer, conv layer, pool layer, activation layer, nonlinearities, activation functions (sigmoid, tanh, relu, swish, softmax), loss functions (mse loss, bce loss, ce loss), logits, probabilities, metrics, overfitting, regularization (l1, l2, elastic, dropout, data augmentation), automl, neural architecture search (nas), transfer learning, fine-tuning, domain adaptation, multilayer perceptron (mlp), feedforward neural network (fnn), fully connected neural network (fcnn), vanishing / exploding gradients (rnn, chain rule, residual / skip connections, lstm, gradient clipping), dilated convolution, casual convolution, dilated casual convolution, temporal convolutional networks, lookahead convolution (depthwise), time delay neural network (tdnn)

> computer vision: resizing, padding, normalization, convolution, kernels, filters, padding, stride, feature maps, feature extraction, backbone, activation functions, average / max pooling, adaptive pooling, fully-connected neural network (linear classifier), softmax, upsampling, transposed convolution, dilated convolution, max pooling, max unpooling, residual / skip connections, depthwise separable convolution,  ||| image classification, object detection (bounding boxes), segmentation (semantic, instance), position / pose estimation, image search, optical character recognition, image captioning, video understanding, subtitles generation, action description, style transfer, image generation / completion ||| alexnet (2012), vgg (2014), resnet (2015), densenet, inception, efficientnet, single shot multibox detector (ssd), you only look once (yolo5), efficientdet, deconvnet, unet, deeplabv3, variational autoencoder (vae, encoder + decoder), generative adversarial network (gan, discriminator + generator), flow-based models, diffusion models, vision transformers (vit), capsnet, convnext, resnext, faster r-cnn, mask r-cnn, detectron2 (facebook) ||| imagenet (1000 classes, 14m images), mscoco, google open images ||| mse loss, vae loss, kl-divergence ||| intersection over union (iou), precision, recall, confusion matrix

> natural language processing: count-based methods, nn-based methods, regexp tokenization, byte-pair encoding tokenization (bpe), normalization (stemming, lemmatization), stopwords, rare words, dictionary size, n-grams, collocations, n-gram language model (lm), hidden markov model (hmm), naive bayes classifier, word representations (embeddings / vectors), one-hot encoding (ohe), bag of words (bow), bag of embeddings (boe), term frequency - inverse document frequency (tf-idf), sliding window, co-occurrence, positive pointwise mutual information (ppmi), matrix factorisation, singular value decomposition (svd), latent semantic analysis (lsa), word2vec (skipgram, cbow, center / context word, negative sampling), glove, fasttext, static / contextualized embeddings, transformers, elmo, bert (roberta, bart), gpt, t5, transformer-xl, xlnet, text augmentation (word dropout, synonyms, 3rd language translation), pruning (heads), adapters (bert), probing classifiers, domain shift check ||| glue, superglue, russian superglue, squad, oscar ||| text classification (toxic, spam, intention), sentiment analysis, spam detection, toxic comments detection, ranking search results, speech / spelling correction, word alignment, machine translation, writing style recognition, text summarization, named entity recognition (ner), dependency parsing, consistency parsing, intention classification, dialogue systems, chat bots, voice assistants, image captioning, text to image, question answering, part-of-speech tagging (pos-tagging) ||| sklearn, nltk, gensim, transformers & datasets (hugging face), youtokentome, sentencepiece, spacy, razdel, matasha, pymorphy2, pymystem3, rnnmorph, fairseq ||| perplexity, bleu, rouge, f1 ||| binary cross entropy loss (bce loss), cross entropy loss (ce loss)

> audio & speech processing: human hearing, time domain, frequency domain, framing, sliding window, sample rate, bit depth, bit rate, resampling, phonemes, acoustic model, language model, sequence to sequence model, autoregressive model, recurrent neural network (rnn), convolutional neural network (cnn), text normalization, n-gram language model, greedy decoding, beam search, punctuation, capitalization, hidden markov model (hmm), griffin-lim (vocoder), neural vocoder, speaker / voice features, speaker embedding (dim reduction + clustering), speaker encoder (x-vector), phoneme embedding, tone / stress embedding, fast fourier transform (fft), short-time fourier transform (stft), mel-scale power spectrum, mel-spectrogram, mel-frequency cepstral coefficients (mfcc), specaugment, wav2vec (facebook), wav2vec 2 (2020), wav2letter (2016), wavenet (deepmind, 2016), wavelets, waveglow, wavegan (parallel), audio spectrogram transformer (ast), tacatron2 (nvidia), jasper (2019), quartznet (2019), contextnet (2020), transducer (rnn-t), conformer (2020), mellotron, autovc, speechsplit, gazev, melgan, pwgan, fastspeech, listen attend spell (las, 2015), deepspeech2 (2015), espnet (end-to-end speech processing), speaker verification to multispeaker text-to-speech (sv2tts), ecapa-tdnn (speaker verification), cycle vae, vqvae, againvc, noisevc, attention embedding, phonetic posteriorgrams, demucs, denoiser, open-unmix ||| cross entropy loss, connectionist temporal classification loss (ctc), rnn-transducer loss (rnn-t), word error rate (wer), character error rate (cer), sentence error rate (ser), ||| audio classification (sex, age, genre, bird), voice activity detection (vad), keyword spotting (kws), spotter (hi siri), end of utterance detection (eou), acoustic / sound event detection (gunshots in noisy environment), language identification, mutli-speaker text to speech, music source separation, speech enhancement (denoising, noise canceling), music generation, guitar amps (distortion), automatic speech recognition (asr), speech to text (stt), text to speech (tts), voice cloning (vc), voice conversion (vc), speaker identification, speaker verification, speaker diarization ||| librosa, torchaudio, soundfile, kaldi, pydap, demucs, open-unmix ||| timit, librispeech, librivox, ted talks, common voice, librespeech, mozilla deep speech, ruslan, russian open speech to text (stt / asr)

> time series: univariate / multivariate, stationary / non-stationary, statistical / nn-based approaches, sliding window, trend, seasonality, autocorrelation, noise, feature extraction / generation / engineering, conv1d, domain specific feature generation, fourier transform, naive forecasting, moving average, differencing, autoregressive (ar), moving average (ma), vector (var, multivariate), integrated (arima), seasonality (sarima), hidden markov model (hmm), bayesian structural time series (bsts), recurrent neural networks (rnn) ||| forecasting, imputation, anomaly detection, pattern search, health (smart watches), stock prices, weather forecast, sunspot activity, births / deaths (retirement planning), web service attacks (anomaly detection), electricity nets failures (anomaly detection) ||| mse loss, huber loss ||| mse, rmse, mae, mape ||| prophet (facebook)

> algorithms: linear regression, ridge regression, lasso regression, logistic regression, softmax regression, support vector machine (svm), kernel trick, k-nearest neighbors (knn), naive bayes, decision trees, ensembles, bagging (voting, mean), stacking (meta model), blending (holdout), boosting (sequential, error correction), bootstrapping (new sample for each tree), random subspace method (new features for each split), random forest, gradient boosting, k-means, density-based spatial clustering of applications with noise (dbscan), gaussian mixture models (gmm), kernel density estimation (kde), agglomerative (hierarchical), singular value decomposition (svd), principal component analysis (pca), latent semantic analysis (lsa), t-distributed stochastic neighbor embedding (t-sne), uniform manifold approximation and projection (umap), isometric mapping (isomap), multidimensional scaling (mds)

> ml design: project life cycle, data mining, exploratory data analysis (eda), missing values (max, mean, neighbors, groups), outliers (iqr, boxplot), feature extraction, feature engineering (log(x), x ** 2, bins, one-hot, mean target encoding, pca), feature selection (l1, weights, importance), features / target interactions (correlation, association, causation), adversarial validation, one-hot encoding, scaling (normalization, standardization) data augmentation, hyperparameter tuning, training, validation (cross-validation, k-folds, stratified k-folds, leave-one-out, holdout, data leakage), evaluation, deployment, monitoring, data drifting

> optimization: parameters (weights), loss / cost function, normal equation, ordinary least squares (ols), gradient descent, underfitting, generalization, overfitting, regularization, l1, l2, elastic, dropout, more data, augmentation, hyperparameters (grid search)

> loss functions: mean squared error (mse, l2 loss), mean absolute error (mae, l1 loss), root mean squared error (rmse, l2), mean squared logarithmic error (msle),  huber loss (smooth mae / l1), log cosh loss, hinge loss (svm), binary cross-entropy loss (bce loss, log loss), cross entropy loss (negative log likelihood), kullback leibler divergence loss (kl-divergence), connectionist temporal classification loss (ctc), rnn-transducer loss (rnn-t), triplet loss, gini impurity (split criterion), information gain (split criterion)

> metrics: mean squared error (mse), mean absolute error (mae), mean absolute percentage error (mape), root mean squared error (rmse), coefficient of determination (r squared), accuracy, true positive rate (tpr), false positive rate (fpr), confusion matrix, precision, recall, micro / macro average, f1-score (harmonic mean), f1-beta-score (precision vs recall), receiver operating characteristics curve (roc), precision-recall curve (prc), area under the curve (auc), roc-auc, pr-auc, silhouette score, completeness score, homogeneity score, v-measure score (f1, completeness & homogeneity), perplexity, bleu score, rouge score, f1-score (bleu & rouge), hit ratio (hr), mean reciprocal rank (mrr), mean average precision (map@k, precison@k, recall@k), normalized discounted cumulative gain (ndcg), intersection over union (iou, jaccard index)

> distance measures: euclidian, manhattan, minkowski, chebyshev, cosine (angle), hamming, jaccard (iou), haversine (sphere), sorensen dice

> combinatorics, probability theory, statistics: bayesian vs frequentist, combinations, permutations, binomial coefficient, random variables (deterministic world, randomness, blackbox), events, probability rules, conditional probability, bayes' theorem, total probability, central tendency, data, dispersion, variables, studies, samples, models, distributions, hypothesis testing, confidence intervals, central limit theorem, empirical rule, maximum likelihood, summary tables, factor analysis, correlations, dummy variables (one-hot encoding), summary statistics, quantiles, quartiles, percentiles, contingency table, min, 1st-3rd quartile, max, range, interquartile range, outliers, pie chart, bar chart, histogram, scatter plot, boxplot, qq plot, icicle plot, dendrogram plot, mosaic plot, confusion matrix, ab tests

> programming: procedure, imperative, functional, data driven, recursion, iterative, dynamic, divide & conquer, object-oriented, parallel, structured, modular

> python: operators (arithmetic, bitwise, logical, comparison, assignment), conditionals (if, ternary operator), data types (int, float, string, tuple, list, dict, set, mutable / immutable), sequences, collections, iterators, statements, expressions, conditionals, loops (while, for), functions, exceptions, code style, map, filter, lambda, enumerate, zip, list / set / dict comprehension, generators, functional programming, object-oriented programming

> data structures & algorithms: time complexity, space complexity, search, sort, scheduling, abstract data types vs data structures, list, array, stack, queue, heap, tree, graph, hash table, string, linear search, binary search, jump search, insertion sort, merge sort, bubble sort, counting sort, quick sort, selection sort, tree traversal, graph traversal, search a substring, hamming distance, suffix tree, prefix function, edit distance, fixed size array, dynamic array, singly / doubly linked list, min / max binary heap, complete / full / perfect binary tree, queue, dequeue, circular queue, priority queue; graph: complete / incomplete, dense / sparse, weighted / unweighted, connected / disconnected, directed / undirected, cyclic / acyclic, bipartite; adjacency matrix, adjacency list, edge list, grid, shortest path, topological sort, network flow, network robustness, minimum spanning tree, strongly connected components, traveling salesman

> math: linear algebra, calculus, statistics, probability theory

> variables: quantitative (continuous, discrete, binary), qualitative (categorical, ordinal, nominal), dependent (outcome, response), independent (predictors, covariates, regressors, features)

> data: structured (tables, dbs), unstructured (text, audio, images, videos) + representation
problems: classification, regression, ranking, clustering, transformation (dimensionality reduction)
classification: binary, multi-class, multi-label, imbalanced
machine learning: supervised, unsupervised, self-supervised, semi-supervised, contrastive, reinforcement, transfer, zero-shot, one-shot, few-shot, online

> regex, sql, python, algorithms, libs, frameworks, devops
math: linear algebra, calculus, statistics, probability theory

> permutations, combinations
> conditional probability, bayes' theorem

> supervised, unsupervised, reinforcement learning
> discrete / continuous, numerical / categorical variables
> structured / unstructured data (tables vs text / audio / images / videos)
> time series, recommender systems

> reinforcement learning: proximal policy optimization (ppo)

> convolutional, recurrent, transformer, mlp-mixer, auto-encoders, generative adversarial, flow-based, diffusion, encoder-decoder, sequence to sequence
